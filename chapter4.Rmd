---
title       : Exploring variation and dependence
description : Variation and dependence are at the heart of Statistics. In fact, without variation the discipline would cease to exist. Correlation is not causation, but why?

--- type:NormalExercise lang:r xp:50 skills:1 key:ea11f25c3e
## Be aware of varying variables

In the previous chapter you learned how to explore the variables in your data by creating summaries and plots out of data objects in R. In fact, you already know quite a bit about how to exlore the variability of different variables.

In this chapter we'll expand on these tools and we'll also look at how to explore the *relationships* between pairs of variables.

*** =instructions
- instruction 1
- instruction 2

*** =hint
- hint 1
- hint 2


*** =pre_exercise_code
```{r}
# load data from web
students2014 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS-data.txt", sep="\t", header=TRUE)
# keep a couple background variables
students2014 <- students2014[,c("sukup","toita","ika","pituus","kenka","kone")]
# recode kone variables missing values as factor levels
students2014$kone <- addNA(students2014$kone)
# keep only rows without missing values
students2014 <- students2014[complete.cases(students2014),]
# integers to numeric
students2014$ika <- as.numeric(students2014$ika)
students2014$pituus <- as.numeric(students2014$pituus)
students2014$kenka <- as.numeric(students2014$kenka)

```

*** =sample_code
```{r}
# students2014 is available


```

*** =solution
```{r}
#solution code here
```

*** =sct
```{r}

test_error()
success_msg("Ok let's get to it!")

```

--- type:NormalExercise lang:r xp:100 skills:1 key:0083627e50
## Standard deviation

Standard deviation is perhaps the most important measure of variability in statistics. For any variable x, the sample standard deviation is the average (euclidean) distance from the mean, $\bar{x}$. So, the closer the values of a variable are to their mean, the smaller the standard deviation.

Mathematically, the sample standard deviation is computed using

$$sd = \sqrt[2]{\frac{1}{n-1} \cdot \Sigma_n (x_n - \bar{x})^2}$$

Standard deviation is very closely related to variance. Variance is standard deviation squared (`^2`), which means that standard deviation equals to the square root (`sqrt()`) of variance.

In R, standard deviation of a numeric vector can be computed using the function `sd()`.


*** =instructions
- Print out a summary of `height`.
- Compute the standard deviation of height, using the variance of height.
- Compute the standard deviation of height, using the function `sd()`.

*** =hint
- `sqrt()` computes the square root. Take advantage of the relationship between variance and standard deviation.


*** =pre_exercise_code
```{r}
# load data from web
students2014 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS-data.txt", sep="\t", header=TRUE)
# keep a couple background variables
students2014 <- students2014[,c("sukup","toita","ika","pituus","kenka","kone")]
# recode kone variables missing values as factor levels
students2014$kone <- addNA(students2014$kone)
# keep only rows without missing values
students2014 <- students2014[complete.cases(students2014),]
# integers to numeric
students2014$ika <- as.numeric(students2014$ika)
students2014$pituus <- as.numeric(students2014$pituus)
students2014$kenka <- as.numeric(students2014$kenka)

height <- students2014$pituus
```

*** =sample_code
```{r}
# height is available

# Print a summary of height
summary(height)

# Compute the variance of height
n <- length(height) # number of observations
var_height <- sum((height - mean(height) )^2) / (n - 1)

# Compute the standard deviation using var_height


# Compute the standard deviation using sd()


```

*** =solution
```{r}
# height is available

# Print a summary of height
summary(height)

# Compute the variance of height
n <- length(height) # number of observations
var_height <- sum((height - mean(height) )^2) / (n - 1)

# Compute the standard deviation using var_height
sqrt(var_height)

# Compute the standard deviation using sd()
sd(height)


```

*** =sct
```{r}

test_function("sqrt", args=c("x"))
test_function("sd", args = c("x"))

test_error()
success_msg("Awsome work!")
```


--- type:NormalExercise lang:r xp:100 skills:1 key:989f75fe50
## Clustering

[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are (in some way) more similar to each other than to those in other groups.

[K-means](https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Clustering/K-Means#Algorithm) is a popular algorithm that performs clustering. Intuitively, in order to split a variable to k clusters, the mean values of k groups are used as *centroids* and the closest centroid defines the cluster. The centroids are found in such a way that the combined distances to the centroid points is the smallest possible. 

In a simple case this is identical to saying that the combined standard deviation inside the defined clusters is the smallest possible.


*** =instructions
- Modify the code to use k-means with two centroids (centers).
- Print out the values of the centers. Can you find interpretations to the clusters? Are the center values lower or higher than you might have expected? Can you think of a possible reason?
- Use `$` to access the vector `cluster` in the object `km`. Assign that vector to the column `cluster` in `students2014`.
- Plot the students heights again but this time color the values by cluster.

*** =hint
- Define the number of clusters using the `centers` argument of `kmeans()`.
- Use `km$cluster` to access the vector of cluster values.
- Copy the code where `plot()` is called but change the `col` argument. You can use the column `cluster` in the data.frame `students2014` just like in the copied code.


*** =pre_exercise_code
```{r}
# load data from web
students2014 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS-data.txt", sep="\t", header=TRUE)
# keep a couple background variables
students2014 <- students2014[,c("sukup","toita","ika","pituus","kenka","kone")]
# recode kone variables missing values as factor levels
students2014$kone <- addNA(students2014$kone)
# keep only rows without missing values
students2014 <- students2014[complete.cases(students2014),]
# integers to numeric
students2014$ika <- as.numeric(students2014$ika)
students2014$pituus <- as.numeric(students2014$pituus)
students2014$kenka <- as.numeric(students2014$kenka)
```

*** =sample_code
```{r}
# students2014 is available

# Trick to plot student height vs constant y = 1
x <- data.frame(height = students2014$pituus,  y = 1)

plot(x, pch = "|", col = students2014$sukup)

# Use k-means algorithm to group the students
# Use only information on their heights
km <- kmeans(students2014$pituus, centers = 1)

# Values of the cluster centroids
km$centers

# Add cluster information to students2014 data
students2014$cluster <- NULL

# Plot the student heights as before
# Color the data points by cluster



```

*** =solution
```{r}
# students2014 is available

# Trick to plot student height vs constant y = 1
x <- data.frame(height = students2014$pituus,  y = 1)

plot(x, pch = "|", col = students2014$sukup)

# Use k-means to group the students by height
km <- kmeans(students2014$pituus, centers = 2)

# Values of the cluster centroids
km$centers

# Add cluster information to students2014
students2014$cluster <- km$cluster

# Plot the student heights as before
# Color the data points by cluster
plot(x, pch = "|", col = km$cluster)


```

*** =sct
```{r}

test_object("students2014")
test_function("plot", args=c("col"))

test_error()
success_msg("Nice work!")

```


--- type:NormalExercise lang:r xp:100 skills:1 key:1d52143e10

## Guess the correlation

Correlation measures the strength of the *linear* relationship between two variables. The easiest way to check if the relationship between two variables might be  interpret as linear is to visualize the relationship with a scatterplot. 

It is then possible to make a reasonable guess of the strength of the linear relationship. In order for you to practise this skill, we have build a game you can play.

*** =instructions
- Use `simulate()` to draw a scatter plot using simulated data. The variables can have linear and nonlinear relationships. Try to guess what the correlation might be.
- Use `show_correlation()` to check how close you got.
- You can experiment as many times as you wish and the simulation will be different each time. Feel free to change `n`, the number of simulated values.

*** =hint
- When you are done experimenting, simply click 'Submit Answer' to move forward.


*** =pre_exercise_code
```{r}
# simulate and plot correlated data
simulate <- function(n = 60) {
  
  if(n > 1e5) stop("That n is quite large. Please choose a smaller n")
  
  # degree (d) of the polynomial
  degree <- sample(1:3, 1)
  
  # x values
  x <- rnorm(n)
  
  # each x^d multiplied by it's coefficient
  # the coefficients are sampled from U(-1, 1)
  X <- sapply(1:degree, function(d) {
    b <- runif(1, min = -1, max = 1)
    b * x^d
  })
  
  # linear equation y = b_1 * x^1 + .. + b_d * x^d + e
  y <- rowSums(X) + rnorm(n)
  
  # scatterplot
  plot(x, y, col = "grey40", type = "p", pch= 20, xlab="", ylab="")
  
  return(list(x = x, y = y))
}

show_correlation <- function(df) {
  x <- df$x
  y <- df$y
  plot(x, y, type = "p", col = "grey40", pch= 20, xlab="", ylab="",
       sub = paste("correlation:", round(cor(x, y),2)))
  abline(lm(y~x), col ="red")
}

```

*** =sample_code
```{r}
# Functions simulate() and show_correlation() are available

# Simulate values and draw a scatterplot
data <- simulate(n = 60)

# What is the correlation?

# Show the linear relationship and correlation
show_correlation(data)

```

*** =solution
```{r}
#solution code here
```

*** =sct
```{r}

test_error()
success_msg("Thanks for playing!")
```
