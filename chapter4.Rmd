---
title       : Exploring variation and dependence
description : Variation and dependence are at the heart of Statistics. In fact, without variation the discipline would cease to exist. Correlation is not causation, but why?

--- type:NormalExercise lang:r xp:50 skills:1

## Be aware of varying variables

In the previous chapter you learned how to explore the variables in your data by creating summaries and plots out of data objects in R. In fact, you already know quite a bit about how to exlore the variability of different variables.

In this chapter we'll expand on these tools and we'll also look at how to explore the *relationships* between pairs of variables.

*** =instructions
- instruction 1
- instruction 2

*** =hint
- hint 1
- hint 2


*** =pre_exercise_code
```{r}
# load data from web
students2014 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS-data.txt", sep="\t", header=TRUE)
# keep a couple background variables
students2014 <- students2014[,c("sukup","toita","ika","pituus","kenka","kone")]
# recode kone variables missing values as factor levels
students2014$kone <- addNA(students2014$kone)
# keep only rows without missing values
students2014 <- students2014[complete.cases(students2014),]
# integers to numeric
students2014$ika <- as.numeric(students2014$ika)
students2014$pituus <- as.numeric(students2014$pituus)
students2014$kenka <- as.numeric(students2014$kenka)

```

*** =sample_code
```{r}
# students2014 is available


```

*** =solution
```{r}
#solution code here
```

*** =sct
```{r}

test_error()
success_msg("Ok let's get to it!")

```

--- type:NormalExercise lang:r xp:100 skills:1

## Standard deviation

Standard deviation is the most important measure of variability in statistics. For a variable x, the sample standard deviation is the average euclidean distance from the mean $\overbrace{x}$.  

Mathematically for n obervations this can be expressed $\frac{1}{n} \cdot \Sigma_n |x_n - \overbrace{x}|}$, where $\Sigma_n$ expresses the sum over the n values. In practise, this is computed using

$$\sqrt[2]{\frac{1}{n-1} \cdot \Sigma (x - \overbrace{x})^2}$$

In R, standard deviation can be computed using the function `sd()`. Standard deviation is very closely related to variance, `var()`. Variance equals standard deviation squared (`^2`), which means that standard deviation equals to the square root (`sqrt()`) of variance.


*** =instructions
- Print out a summary of `height`.
- Use `sqrt()` to compute the standard deviation of height.
- Print out the standard deviation.
- Use `sd()` on `height` and compare the result to `sd_height`.

*** =hint
- Use `sqrt()` on the object `var_height`.
- Use `sd()` on the object `height`.
- The `==` performs a logical comparison.


*** =pre_exercise_code
```{r}
# load data from web
students2014 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS-data.txt", sep="\t", header=TRUE)
# keep a couple background variables
students2014 <- students2014[,c("sukup","toita","ika","pituus","kenka","kone")]
# recode kone variables missing values as factor levels
students2014$kone <- addNA(students2014$kone)
# keep only rows without missing values
students2014 <- students2014[complete.cases(students2014),]
# integers to numeric
students2014$ika <- as.numeric(students2014$ika)
students2014$pituus <- as.numeric(students2014$pituus)
students2014$kenka <- as.numeric(students2014$kenka)

height <- students2014$pituus
```

*** =sample_code
```{r}
# height is available

# Print a summary of height
summary(height)

# Compute the variance of height
n <- length(height)
var_height <- sum( (height - mean(height) )^2 ) / (n-1)

# Compute the standard deviation using var_height
sd_height <- NULL

# Print the standard deviation of height


# Check that the result is the same as with sd()
sd_height == 1


```

*** =solution
```{r}
# height is available
n <- length(height)

# Print a summary of height
summary(height)

# Compute the variance of height
var_height <- sum( (height - mean(height) )^2 ) / (n-1)

# Compute the standard deviation using var_height
sd_height <- sqrt(var_height)

# Print the standard deviation of height
sd_height

# Check that the result is the same as with sd()
sd_height == sd(height)


```

*** =sct
```{r}

test_object("sd_height")
test_function("sqrt", args=c("x"))
test_student_typed("sd(height)")

test_error()
success_msg("Awsome work!")
```


--- type:NormalExercise lang:r xp:100 skills:1

## Clustering

[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are (in some way) more similar to each other than to those in other groups.

K-means is a popular algorithm that performs clustering. Intuitively, in order to split a variable to k clusters, the mean values of k groups are used as *centroids* and the closest centroid defines the cluster.

The centroids are found in such a way that the combined distances to the centroid points is the smallest possible. In a simple case this is identical to saying that the combined standard deviation inside the clusters is the smallest possible.


*** =instructions
- Modify the code to use k-means with two centroids.
- Print out the values of the centroids. Are they lower or higher than expected? Can you think of a possible reason?
- Use `$` to access the vector `cluster` in the `km` object. Assign the vector to the column `cluster` in `students2014`.
- Plot the students heights again but this time color the values by cluster.

*** =hint
- Define the number of clusters using the `centers` argument of `kmeans()`.
- Use `km$cluster` to access the vector of cluster values.
- Copy the code where `plot()` is called but change the `col` argument. You can use the new column `cluster` in the data.frame `students2014` just like in the copied code.


*** =pre_exercise_code
```{r}
# load data from web
students2014 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS-data.txt", sep="\t", header=TRUE)
# keep a couple background variables
students2014 <- students2014[,c("sukup","toita","ika","pituus","kenka","kone")]
# recode kone variables missing values as factor levels
students2014$kone <- addNA(students2014$kone)
# keep only rows without missing values
students2014 <- students2014[complete.cases(students2014),]
# integers to numeric
students2014$ika <- as.numeric(students2014$ika)
students2014$pituus <- as.numeric(students2014$pituus)
students2014$kenka <- as.numeric(students2014$kenka)
```

*** =sample_code
```{r}
# students2014 is available

# Trick to plot student height vs constant y = 1
x <- data.frame(height = students2014$pituus,  y = 1)

plot(x, pch = "|", col = students2014$sukup)

# Use k-means algorithm to group the students
# Use only information on their heights
km <- kmeans(students2014$pituus, centers = 1)

# Values of the cluster centroids
km$centers

# Add cluster information to students2014 data
students2014$cluster <- NULL

# Plot the student heights as before
# Color the data points by cluster




```

*** =solution
```{r}
# students2014 is available

# Trick to plot student height vs constant y = 1
x <- data.frame(height = students2014$pituus,  y = 1)

plot(x, pch = "|", col = students2014$sukup)

# Use k-means to group the students by height
km <- kmeans(students2014$pituus, centers = 2)

# Values of the cluster centroids
km$centers

# Add cluster information to students2014
students2014$cluster <- km$cluster

# Plot the student heights as before
# Color the data points by cluster
plot(x, pch = "|", col = km$cluster)



```

*** =sct
```{r}

test_object("students2014")
test_function("plot", args=c("col"))

test_error()
success_msg("Nice work!")

```


--- type:NormalExercise lang:r xp:100 skills:1 key:1d52143e10

## Guess the correlation

Correlation measures the strength of the *linear* relationship between two variables. The easiest way to check if the relationship between two variables might be  interpret as linear is to visualize the relationship with a scatterplot. 

It is then possible to make a reasonable guess of the strength of the linear relationship. In order for you to practise this skill, we have build a game you can play.

*** =instructions
- Use `simulate()` to draw a scatter plot using simulated data. The variables can have linear and nonlinear relationships. Try to guess what the correlation might be.
- Use `show_correlation()` to check how close you got.
- You can experiment as many times as you wish and the simulation will be different each time. Feel free to change `n`, the number of simulated values.

*** =hint
- When you are done experimenting, simply click 'Submit Answer' to move forward.


*** =pre_exercise_code
```{r}
# simulate and plot correlated data
simulate <- function(n = 60) {
  
  if(n > 1e5) stop("That n is quite large. Please choose a smaller n")
  
  # degree (d) of the polynomial
  degree <- sample(1:3, 1)
  
  # x values
  x <- rnorm(n)
  
  # each x^d multiplied by it's coefficient
  # the coefficients are sampled from U(-1, 1)
  X <- sapply(1:degree, function(d) {
    b <- runif(1, min = -1, max = 1)
    b * x^d
  })
  
  # linear equation y = b_1 * x^1 + .. + b_d * x^d + e
  y <- rowSums(X) + rnorm(n)
  
  # scatterplot
  plot(x, y, col = "grey40", type = "p", pch= 20, xlab="", ylab="")
  
  return(list(x = x, y = y))
}

show_correlation <- function(df) {
  x <- df$x
  y <- df$y
  plot(x, y, type = "p", col = "grey40", pch= 20, xlab="", ylab="",
       sub = paste("correlation:", round(cor(x, y),2)))
  abline(lm(y~x), col ="red")
}

```

*** =sample_code
```{r}
# Functions simulate() and show_correlation() are available

# Simulate values and draw a scatterplot
data <- simulate(n = 60)

# What is the correlation?

# Show the linear relationship and correlation
show_correlation(data)

```

*** =solution
```{r}
#solution code here
```

*** =sct
```{r}

test_error()
success_msg("Thanks for playing!")
```
